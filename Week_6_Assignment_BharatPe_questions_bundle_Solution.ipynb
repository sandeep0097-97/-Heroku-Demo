{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sandeep0097-97/-Heroku-Demo/blob/main/Week_6_Assignment_BharatPe_questions_bundle_Solution.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Guidelines for assignment:\n",
        "\n",
        "1. Create a copy of this notebook in your drive and write down the answers to each question in your own words. Avoid plagiarism.\n",
        "2. Record a video of yourself answering only the 15 ML questions in a manner, (with your camera on) you would in an interview. The intention is not to read out the answer you have written. Understand the concept and explain it in your words as you would to an interviewer. You can use Zoom or similar applications to record your video.\n",
        "3. Upload the recorded video to your YouTube account as an **unlisted** video and share its link in the notebook along with solutions.\n",
        "You can refer to [this video](https://youtu.be/JOr7JluzEOM) for steps to unlist your video."
      ],
      "metadata": {
        "id": "zRy1zWXy4_VF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Case Study Question:**\n",
        "\n",
        "1. Summarise your understanding about the company and its operations. What is the business model here?\n",
        "\n",
        "2. Conduct a SWOT (Strengths, Weaknesses, Opportunities, Threats) analysis of the firm with respect to Fintech industry.\n",
        "\n",
        "3. What will be your approach to build a fraud detection system to detect fraudulent transactions in BharatPe platform?\n",
        "\n",
        "4. What are the features you would require to design this system?\n",
        "\n",
        "5. How do you measure the performance of the system?"
      ],
      "metadata": {
        "id": "ZaN7y7eFKy0v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Answer the following 15 questions:"
      ],
      "metadata": {
        "id": "-toTqfujU_12"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1. What are the  assumptions of  Linear Regression?**"
      ],
      "metadata": {
        "id": "XIFYb0sZK-1J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The assumptions of Linear Regression are:\n",
        "1. The relation between the dependent and independent variables should be almost linear.\n",
        "2. Mean of residuals should be zero or close to 0 as much as possible.\n",
        "3. There should be homoscedasticity or equal variance in a regression model. This assumption means that the variance around the regression line is the same for all values of the predictor variable (X).\n",
        "4. There should not be multicollinearity in regression model. Multicollinearity generally occurs when there are high correlations between two or more independent variables.\n"
      ],
      "metadata": {
        "id": "lsRgIVpaqcB1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2. Explain how  Decision Tree Algorithm works? Also explain the criteria on which splitting takes place?**"
      ],
      "metadata": {
        "id": "TUfMdQ4BLVRM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Decision Tree is a Supervised learning technique that can be used for both classification and Regression problems. It is a tree-structured classifier, where internal nodes represent the features of a dataset, branches represent the decision rules and each leaf node represents the outcome.\n",
        "\n",
        "At the beginning, we consider the whole training set as the root. This algorithm compares the values of root attribute with the record (real dataset) attribute and, based on the comparison, follows the branch and jumps to the next node. For the next node, the algorithm again compares the attribute value with the other sub-nodes and move further. It continues the process until it reaches the leaf node of the tree.  \n",
        "\n",
        "While implementing a Decision tree, the main issue arises that how to select the best attribute for the root node and for sub-nodes. There are 2 methods which acts as popular splitting criterion for decision tree models:-\n",
        "\n",
        "\n",
        "1.   *Information Gain:* Information gain is the measurement of changes in entropy after the segmentation of a dataset based on an attribute. It calculates how much information a feature provides us about a class. A decision tree algorithm always tries to maximize the value of information gain, and a node/attribute having the highest information gain is split first. Entropy is a metric to measure the impurity in a given attribute. It specifies randomness in data. The higher the entropy more the information content.\n",
        "2.   *Gini index*: Gini Index is a metric to measure how often a randomly chosen element would be incorrectly identified.It means an attribute with lower Gini index should be preferred.\n",
        "\n"
      ],
      "metadata": {
        "id": "R9YtI5cRrIyc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **3. What is  Random Forest algorithm? How does random forest  reduce overfitting?**"
      ],
      "metadata": {
        "id": "4Psn07f8LavD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Random Forest is based on the concept of ensemble learning, which is a process of combining multiple classifiers to solve a complex problem and to improve the performance of the model. Random Forest is a classifier that contains a number of decision trees on various subsets of the given dataset and takes the average to improve the predictive accuracy of that dataset. Instead of relying on one decision tree, the random forest takes the prediction from each tree and based on the majority votes of predictions, and it predicts the final output.\n",
        "\n",
        "The greater number of trees in the forest leads to higher accuracy and prevents the problem of overfitting."
      ],
      "metadata": {
        "id": "x9BsM7NRzBb7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **4. What is the difference between Precision and Recall? Explain with example**"
      ],
      "metadata": {
        "id": "jtKqAJkwLnxz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Precision is calculated by dividing the true positives by anything that was predicted as a positive. **It is the number of instances that are relevant, out of the total instances the model retrieved.**\n",
        "Recall (or True Positive Rate) is calculated by dividing the true positives by anything that should have been predicted as positive. **It is the  number of instances which the model correctly identified as relevant out of the total relevant instances.**\n",
        "\n",
        "Example: Fishing with a net. If you use a wide net, and catch 80 of 100 total fish in a lake. That’s 80% recall. But you also get 80 rocks in your net. That means 50% precision, half of the net’s contents is junk.\n",
        "\n",
        "You could use a smaller net and target one pocket of the lake where there are lots of fish and no rocks, but you might only get 20 of the fish in order to get 0 rocks. That is 20% recall and 100% precision.\n",
        "\n"
      ],
      "metadata": {
        "id": "36GcrCoY_xgf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **5. What is K Means?  How do you find optimal value of k  in K Means Clustering?**"
      ],
      "metadata": {
        "id": "IxSqWBOQLtJD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "K-Means Clustering is an Unsupervised Learning algorithm, which groups the unlabeled dataset into different clusters or groups having similar properties. Here K defines the number of pre-defined clusters that need to be created in the process. It is a centroid-based algorithm, where each cluster is associated with a centroid. The main aim of this algorithm is to minimize the sum of distances between the data point and their corresponding clusters.\n",
        "\n",
        "The popular method to determine the optimal value of K is elbow method. This method uses the concept of WCSS value. WCSS stands for Within Cluster Sum of Squares, which defines the total variations within a cluster. The elbow method follows the below steps:\n",
        "\n",
        "* It executes the K-means clustering on a given dataset for different K values (ranges from 1-10).\n",
        "* For each value of K, calculates the WCSS value.\n",
        "* Plots a curve between calculated WCSS values and the number of clusters K.\n",
        "* The sharp point of bend or a point of the plot looks like an arm, then that point is considered as the best value of K."
      ],
      "metadata": {
        "id": "JUOT2WiFECx4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **6. What is Confusion Matrix?**"
      ],
      "metadata": {
        "id": "j0nC1EquMCvL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is a performance measurement for machine learning classification problem where output can be two or more classes. It is a table with 4 different combinations of predicted and actual values. It is extremely useful for measuring Recall, Precision, Specificity, Accuracy, and most importantly AUC-ROC curves.\n",
        "\n",
        "* A good model is one which has high TP and TN rates, while low FP and FN rates.\n",
        "* If you have an imbalanced dataset to work with, it’s always better to use confusion matrix as your evaluation criteria for your machine learning model."
      ],
      "metadata": {
        "id": "i-k4ZAYBLWLa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **7. What is random sampling? Give some examples of some random sampling techniques.**"
      ],
      "metadata": {
        "id": "jG4TND7QMEy7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random sampling simply describes when every element in a population has an equal chance of being chosen for the sample. There are four primary random sampling methods:\n",
        "\n",
        "\n",
        "1.   Simple Random Sampling: In simple random sampling technique, every item in the population has an equal and likely chance of being selected in the sample.\n",
        "\n",
        "2.   Systematic Sampling: In the systematic sampling method, the items are selected from the target population by selecting the random selection point and selecting the other methods after a fixed sample interval.\n",
        "3. Stratified Sampling: In a stratified sampling method, the total population is divided into smaller groups to complete the sampling process. The small group is formed based on a few characteristics in the population. After separating the population into a smaller group, the statisticians randomly select the sample.\n",
        "4. Clustered Sampling: In the clustered sampling method, the cluster or group of people are formed from the population set. The group has similar significatory characteristics. Also, they have an equal chance of being a part of the sample.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "CUupObdZNB9i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **8. What is the difference between tokenisation, lemmatization and stemming?**"
      ],
      "metadata": {
        "id": "hb8VgtsoMMR1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenization is the process of breaking down the given text in natural language processing into the smallest unit in a sentence called a token. Punctuation marks, words, and numbers can be considered tokens.\n",
        "\n",
        "Stemming is the process of finding the root of words.With stemming, words are reduced to their word stems. A word stem need not be the same root as a dictionary-based morphological root, it just is an equal to or smaller form of the word.\n",
        "\n",
        "Lemmatization is the process of finding the form of the related word in the dictionary.The aim of lemmatization, like stemming, is to reduce inflectional forms to a common base form. As opposed to stemming, lemmatization does not simply chop off inflections. Instead, it uses lexical knowledge bases to get the correct base forms of words."
      ],
      "metadata": {
        "id": "bEW5e2TjYLib"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **9. How do you handle imbalance data? Mention some techniques.**"
      ],
      "metadata": {
        "id": "79n0bqAIMOCl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imbalanced data refers to those types of datasets where the target class has an uneven distribution of observations, i.e one class label has a very high number of observations and the other has a very low number of observations.\n",
        "\n",
        "Some of the few techniques to deal with imbalance data are:\n",
        "1. Choose Proper Evaluation Metric: For an imbalanced class dataset F1 score is a more appropriate metric. F1 score keeps the balance between precision and recall and improves the score only if the classifier identifies more of a certain class correctly.\n",
        "\n",
        "2. Resampling (Oversampling and Undersampling): This technique is used to upsample or downsample the minority or majority class. When we are using an imbalanced dataset, we can oversample the minority class using replacement. This technique is called oversampling. Similarly, we can randomly delete rows from the majority class to match them with the minority class which is called undersampling. After sampling the data we can get a balanced dataset for both majority and minority classes. So, when both classes have a similar number of records present in the dataset, we can assume that the classifier will give equal importance to both classes.\n",
        "\n",
        "3. SMOTE: Synthetic Minority Oversampling Technique or SMOTE is another technique to oversample the minority class.  In SMOTE new instances are synthesized from the existing data. SMOTE looks into minority class instances and use k nearest neighbor to select a random nearest neighbor, and a synthetic instance is created randomly in feature space.\n",
        "\n"
      ],
      "metadata": {
        "id": "SRTLZh7UcJHK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **10. What is univariate and bivariate analysis? Mention some its techniques.**"
      ],
      "metadata": {
        "id": "98HHV5PzMTvS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Univariate Analysis: Uni means one and variate means variable, so in univariate analysis, there is only one dependable variable. The objective of univariate analysis is to derive the data, define and summarize it, and analyze the pattern present in it. In a dataset, it explores each variable separately. It is possible for two kinds of variables- Categorical and Numerical.\n",
        "Some of its techniques are:\n",
        "* Frequency Distribution Table\n",
        "* Bar Charts\n",
        "* Histograms\n",
        "* Pie Charts\n",
        "\n",
        "\n",
        "Bivariate Analysis: Bi means two and variate means variable, so here there are two variables. The analysis is related to cause and the relationship between the two variables. Some of its techniques are:\n",
        "*   Scatter Plot\n",
        "* Linear Correlation\n",
        "* Chi-square Test\n",
        "* ANALYSIS OF VARIANCE (ANOVA)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "472WQeSUgTLk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **11. Describe Hypothesis Testing. How is the statistical significance of an insight assessed? What is Alpha in hypothesis testing?**"
      ],
      "metadata": {
        "id": "rRFTCJf6Mqh7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A hypothesis is often described as an “educated guess” about a specific parameter or population. Once it is defined, one can collect data to determine whether it provides enough evidence that the hypothesis is true.\n",
        "\n",
        "In hypothesis testing, two mutually exclusive statements about a parameter or population (hypotheses) are evaluated to decide which statement is best supported by sample data.\n",
        "\n",
        "To assess statistical significance, you would use hypothesis testing. The null hypothesis and alternate hypothesis would be stated first. Second, you’d calculate the p-value, which is the likelihood of getting the test’s observed findings if the null hypothesis is true. Finally, you would select the threshold of significance (alpha) and reject the null hypothesis if the p-value is smaller than the alpha — in other words, the result is statistically significant.\n",
        "\n",
        "Alpha is a threshold value used to judge whether a test statistic is statistically significant. It is chosen by the researcher. Alpha represents an acceptable probability of a Type I error in a statistical test. Because alpha corresponds to a probability, it can range from 0 to 1."
      ],
      "metadata": {
        "id": "uAXYUmh9h17D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **12. What is  the difference between Label encoding and One Hot Encoding?**"
      ],
      "metadata": {
        "id": "NGekudS4Mtug"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Label Encoding is a popular encoding technique for handling categorical variables. In this technique, each label is assigned a unique integer based on alphabetical ordering.\n",
        "\n",
        "One-Hot Encoding  simply creates additional features based on the number of unique values in the categorical feature. Every unique value in the category will be added as a feature. One-Hot Encoding is the process of creating dummy variables. In this encoding technique, each category is represented as a one-hot vector.\n",
        "\n",
        "Example - https://www.naukri.com/learning/articles/one-hot-encoding-vs-label-encoding/"
      ],
      "metadata": {
        "id": "sY8iTFmUPbwE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **13. What is conditional probability? If I call 2 Ubers and 3 Lyfts, what is the probability that all the Lyfts arrive first?**"
      ],
      "metadata": {
        "id": "uVqTzVRAM7mj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Conditional probability is known as the possibility of an event or outcome happening, based on the existence of a previous event or outcome. It is calculated by multiplying the probability of the preceding event by the renewed probability of the succeeding, or conditional, event.\n",
        "\n",
        "For the given problem, using conditional probability:\n",
        "\n",
        "* probability that the first car is Lyft = 3/5\n",
        "* probability that the second car is Lyft = 2/4\n",
        "* probability that the third car is Lyft = 1/3\n",
        "* Therefore, probability that all the Lyfts arrive first = (3/5) * (2/4) * (1/3) = 1/10"
      ],
      "metadata": {
        "id": "MJ84J0sCQier"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **14. What are Outliers and how do you treat them?**"
      ],
      "metadata": {
        "id": "iSD3IFhUNIuM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Outliers are those data points that are significantly different from the rest of the dataset. They are often abnormal observations that skew the data distribution, and arise due to inconsistent data entry, or erroneous observations. To ensure that the trained model generalizes well to the valid range of test inputs, it’s important to detect and remove outliers.\n",
        "\n",
        "Below are some of the methods of treating the outliers\n",
        "\n",
        "* Trimming/removing the outlier - In this technique, we remove the outliers from the dataset.\n",
        "* Quantile based flooring and capping - In this technique, the outlier is capped at a certain value above the 90th percentile value or floored at a factor below the 10th percentile value.\n",
        "* Mean/Median imputation - As the mean value is highly influenced by the outliers, it is advised to replace the outliers with the median value."
      ],
      "metadata": {
        "id": "aSgmKfGfTOEi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **15. What is Naive Bayes Classifier Algorithm. What do you mean by ‘Naive’ in a Naive Bayes?**"
      ],
      "metadata": {
        "id": "hjmlljMFNSWT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Naïve Bayes algorithm is a supervised learning algorithm, which is based on Bayes theorem and used for solving classification problems. It is a probabilistic classifier, which means it predicts on the basis of the probability of an object. It is mainly used in text classification that includes a high-dimensional training dataset.\n",
        "\n",
        "It is called Naïve because it assumes that the occurrence of a certain feature is independent of the occurrence of other features. Such as if the fruit is identified on the bases of color, shape, and taste, then red, spherical, and sweet fruit is recognized as an apple. Hence each feature individually contributes to identify that it is an apple without depending on each other."
      ],
      "metadata": {
        "id": "SIUUlj9xUMgr"
      }
    }
  ]
}